{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation in PyTorch\n",
    "\n",
    "Modern deep learning frameworks rely heavily on *automatic differentiation*, a technique that efficiently computes gradients for optimizing complex models. PyTorch’s `autograd` module provides a powerful and flexible way to perform automatic differentiation, enabling gradient-based optimization with minimal effort. Whether you're training neural networks, computing derivatives for custom functions, or performing higher-order differentiation, `autograd` makes it seamless and effortless. In this tutorial, we'll explore how PyTorch tracks computations, calculates gradients using `.backward()` and `torch.autograd.grad()`, and handles multiple gradient computations. By the end, you'll have a clear understanding of how automatic differentiation works in PyTorch and how to apply it effectively in your own projects. The working knowledge you develop from this tutorial will also play a critical role in learning the implementation of physics-informed loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Autograd - Computing Gradients using `.backward()`\n",
    "\n",
    "PyTorch’s `autograd` module provides automatic differentiation, allowing gradients to be computed effortlessly for tensor operations. This is particularly useful for optimization tasks, such as training deep learning models (we already know, sort of), or as we will see shortly for training physics informed models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, to enable PyTorch to track operations and compute gradients, set `requires_grad=True` when defining a tensor. This will trigger PyTorch to keep track of all operations performed on `x` to facilitate differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once `requires_grad=True` is set, any operations on the tensor are recorded for gradient computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    y = torch.exp(-2.0 * x)\n",
    "    return (1.0 - y) / (1.0 + y)\n",
    "\n",
    "y = tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, PyTorch builds a computational graph connecting `x` and `y`. The system will use this graph to compute derivatives when needed. To compute the derivative of `y` with respect to `x`, you may simply call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4200)\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)    # Should print 0.42, since dy/dx = 1 - tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the derivative $dy/dx$ is stored in `x.grad`, after executing `.backward()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can compute the gradient of multivariate functions in the same way. For instance, let's say that you are interested in computing the gradient of the (squared) Euclidean norm $f(x)=x_1^2 + x_2^2 + \\cdots + x_n^2$. The following code will compute the gradient of the function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1589, 0.7113, 0.5476, 0.9500, 0.0543], requires_grad=True)\n",
      "tensor([0.3177, 1.4226, 1.0952, 1.9001, 0.1087])\n",
      "tensor([0.3177, 1.4226, 1.0952, 1.9001, 0.1087], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def norm(x):\n",
    "    return torch.sum(x**2)\n",
    "\n",
    "x = 2*torch.rand(5) - 1   # a 5-dimensional random vector with elements ranging between -1 and 1.\n",
    "x.requires_grad_()      # another way of setting requires_grad\n",
    "y = norm(x)\n",
    "y.backward()\n",
    "print(x)\n",
    "print(x.grad)\n",
    "print(2*x)            # analytic gradient. `x.grad` should be the same as this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Notes About the `.backward()` Method\n",
    "\n",
    "There are a few important things to remember about the way we computed gradient using the `.backward()` method. First, once `.backward()` is executed, the computational graph connecting `x` and `y` to backpropagate the gradient is automatically deleted to save memory. For example, if you run `.backward()` one more time like in the following, you will get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\mwn4yc\\Anaconda3\\envs\\padl\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mwn4yc\\Anaconda3\\envs\\padl\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "y.backward()        # this line should return ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the error message at the bottom indicates, PyTorch returns the error because it has freed (removed from the memory) the computational graph already, and hence, you can no longer execute the backpropagation operation.\n",
    "\n",
    "In case you need to repeat multiple backpropagation operations on the same variable, you must turn on `retain_graph=True` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n",
      "tensor(24.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3\n",
    "\n",
    "y.backward(retain_graph=True)    # this will prevent PyTorch from freeing the computational graph\n",
    "print(x.grad)               # should return 12, because dy/dx = 3*(x**2)\n",
    "\n",
    "y.backward()                    # computes the gradient again\n",
    "print(x.grad)               # should return 24, I'll explain why..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the above that the output of the second round backpropagation is doubled. This is because we didn't reset the derivative and PyTorch, by default, keeps the results from the previous calculation and accumulate new ones on top of them. If you want to clear the derivatives and redo the calculation, you should call `x.grad.zero_()` beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n",
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3\n",
    "\n",
    "y.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.zero_()    # observe the difference with the addition of this line\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Higher Order Derivatives using the `.backward()` method\n",
    "\n",
    "If everything above makes sense, computing the higher order derivatives using the `backward()` method shouldn't be terribly complicated. Let's see the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., grad_fn=<CloneBackward0>)\n",
      "tensor(2., grad_fn=<ZeroBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "y.backward(create_graph=True)    # create_graph is used instead of retain_graph\n",
    "dydx = x.grad.clone()       # Make a copy of x.grad\n",
    "\n",
    "x.grad.zero_()             # reset grad\n",
    "\n",
    "dydx.backward()         # compute d^2y/dx^2 using the same graph\n",
    "d2ydx2 = x.grad\n",
    "\n",
    "print(dydx)             # This should be 4 because dy/dx = 2*x\n",
    "print(d2ydx2)           # This should be 2 because d^2y/dx^2 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Use of Autograd - Computing Gradients using `torch.autograd.grad`\n",
    "\n",
    "PyTorch provides another way to compute gradients using `torch.autograd.grad()`, which allows **more flexibility** than `.backward()`. In a nutshell, you can think of `.backward()` as implementing *backpropagation*, which is a **special case** of automatic differentiation designed for computing gradients for optimization. In contrast, `torch.autograd.grad()` provides more general *automatic differentiation*, allowing computation of derivatives without modifying `.grad` and handling multiple outputs.\n",
    "\n",
    "Let's parse what all this means by using the following examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple example demonstrating the `torch.autograd.grad()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(8.)\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "f = x*y + y**2\n",
    "\n",
    "grads = torch.autograd.grad(f, (x, y))    # this is how you use torch.autograd.grad\n",
    "\n",
    "print(grads[0])   # should print df/dx, which is 3, because df/dx = y\n",
    "print(grads[1])   # should print df/dy, which is 8, because df/dy = x + 2*y\n",
    "print(x.grad, y.grad)  # prints None for both. I'll explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is what you would've done with the `.backward()` method for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "f = x*y + y**2\n",
    "\n",
    "f.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in case of the `torch.autograd.grad()` method, the gradients are outputed as a separate output than `x.grad` and `y.grad`. In fact, we just saw that runing the line `print(x.grad, y.grad)` printed `None None`.\n",
    "\n",
    "So, one of the major differences between the two method can be said that...\n",
    "- `.backward()`: Computes gradients and **stores them** in the `.grad` attribute of tensors.\n",
    "- `torch.autograd.grad()`: Returns gradients **without modifying** `.grad`, useful when you need gradients without affecting the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here's a more powerful use case for `torch.autograd.grad()`, which is the computation of derivatives for non-scalar outputs.\n",
    "\n",
    "Consider a situation where we have a model that takes, just for simplicity, one input variable and spits out one output variable. Suppose that we have a batch of inputs and we are going to apply the model to these inputs to produce a batch of outputs. This is a typical scenario as we've seen in the previous lab sessions. In this case, we have a batch input and a batch output and the function (model) is no longer a scalar-valued function. Hence, the following code will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m8\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)   \u001b[38;5;66;03m# a batch of 8 samples, each a scalar\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m model(x)                            \u001b[38;5;66;03m# this will return a batch of 8 outputs, each corresponding to an element of x\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m y\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\mwn4yc\\Anaconda3\\envs\\padl\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mwn4yc\\Anaconda3\\envs\\padl\\Lib\\site-packages\\torch\\autograd\\__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    235\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    236\u001b[0m     (inputs,)\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    243\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 244\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\mwn4yc\\Anaconda3\\envs\\padl\\Lib\\site-packages\\torch\\autograd\\__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         )\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m    121\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "def model(x):\n",
    "    return x**2    # A toy model for demo. This could be a neural network.\n",
    "\n",
    "x = torch.rand(8, requires_grad=True)   # a batch of 8 samples, each a scalar\n",
    "y = model(x)                            # this will return a batch of 8 outputs, each corresponding to an element of x\n",
    "\n",
    "y.backward()              # this will return ERROR, because the backward() method is not for non-scalar outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, using `torch.autograd.grad()`, we can compute the derivatives without a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1.6904, 0.8537, 1.5016, 0.3693, 0.5401, 0.0414, 0.1508, 1.6516]),)\n",
      "tensor([1.6904, 0.8537, 1.5016, 0.3693, 0.5401, 0.0414, 0.1508, 1.6516],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(8, requires_grad=True)\n",
    "y = model(x)\n",
    "\n",
    "# note the `grad_outputs` argument. I'll explain it shortly...\n",
    "grads = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y))\n",
    "\n",
    "print(grads)\n",
    "print(2*x)      # analytical gradient. prediction must be the same as this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, note that `grad_outputs` argument is added, with a tensor of the same size as `y` and filled with ones. There's a lot to unpack about this one actually, so bear with me.\n",
    "\n",
    "First, `grad_outputs` should be a sequence of length matching `y` (the output). Given `grad_outputs`, `torch.autograd.grad` will compute what is called *vector-Jacobian product* or *vjp*, which is defined as:\n",
    "\n",
    "$\n",
    "\\mathbf{J}^\\top\\mathbf{v} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_1} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial y_1}{\\partial x_n} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "where $\\mathbf{v}$ is the vector specified for `grad_outputs`. Therefore, if we set `grad_outputs` or $\\mathbf{v}$ to be the vector of the same size as `y` and filled with ones, we are effectively computing:\n",
    "\n",
    "$\n",
    "\\text{grads[i]} = \\sum_{j=1}^m \\frac{\\partial y_j}{\\partial x_i}\n",
    "$\n",
    "\n",
    "which equals to $\\text{grads[i]} = \\frac{\\partial y_i}{\\partial x_i}$ in our case, because $\\frac{\\partial y_j}{\\partial x_i}=0$ if $i \\neq j$ (Note in our example above, `y` was element-wise squares of `x`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, this may sound like a redundant process of computing the gradient. However, in the actual implementation, PyTorch never constructs the Jacobian explicitly, but instead, calculates VJP directly. So the actual computational load does not increase.\n",
    "\n",
    "This trick above is what we are going to use very frequently for the implementation of PINN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Jacobian\n",
    "\n",
    "Of course, if in any case, if you do need the actual Jacobian, there is also a way. Consider a function $\\mathbf{f}:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$, given by\n",
    "\n",
    "$ \\mathbf{f}\\left(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\right) = \n",
    "\\begin{bmatrix} f_1(x,y) \\\\ f_2(x,y) \\end{bmatrix} =\n",
    "\\begin{bmatrix} x^2y \\\\ 5x+\\sin y \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Then the Jacobian of $\\mathbf{f}$ is obtained as:\n",
    "\n",
    "$\n",
    "\\mathbf{J}_\\mathbf{f}(x,y) =\n",
    "\\begin{bmatrix}\n",
    "    \\nabla f_1^\\top \\\\\n",
    "    \\nabla f_2^\\top\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
    "    \\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    2xy & x^2 \\\\\n",
    "    5 & \\cos y\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "For the full Jacobian (as opposed to VJP), we can use `torch.autograd.functional.jacobian()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.2832, 11.0000], grad_fn=<StackBackward0>)\n",
      "tensor([[ 6.2832e+00,  4.0000e+00],\n",
      "        [ 5.0000e+00, -4.3711e-08]])\n",
      "tensor([[ 6.2832e+00,  4.0000e+00],\n",
      "        [ 5.0000e+00, -4.3711e-08]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(torch.pi/2, requires_grad=True)\n",
    "\n",
    "def func(xy):\n",
    "    x, y = xy\n",
    "    return torch.stack([\n",
    "        (x**2)*y,\n",
    "        5*x + torch.sin(y)\n",
    "    ])\n",
    "\n",
    "J = torch.tensor([\n",
    "    [2*x*y, x**2],\n",
    "    [5, torch.cos(y)]\n",
    "])\n",
    "\n",
    "xy = torch.stack([x,y])\n",
    "\n",
    "grads = torch.autograd.functional.jacobian(func, xy)\n",
    "\n",
    "print(f)\n",
    "print(J)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, even though it is possible to compute the full Jacobian, the computational load can snow ball pretty quickly, as the dimensions $m$ and $n$ grow. If you can avoid computing the full Jacobian, but instead VJP, you should do so as much as you could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this session, we saw different ways of computing gradients and other derivatives in PyTorch. Largely, we compared the `.backward()` method and the `torch.autograd.grad()` method. I know it's a lot of information to process, but here's a quick summary of what we learned in this session:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | `tensor.backward()` | `torch.autograd.grad()` |\n",
    "| ------- | ------------------- | ----------------------- |\n",
    "| Computes gradient? | ✅ Yes | ✅ Yes |\n",
    "| Stores gradient in `.grad`? | ✅ Yes | ❌ No (returns as output) |\n",
    "| Works on scalar outputs? | ✅ Yes | ✅ Yes |\n",
    "| Works on non-scalar outputs? | ❌ No (must provide `gradient=` argument) | ✅ Yes (must specify `grad_outputs`) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should you use `torch.autograd.grad()` instead of `.backward()`?\n",
    "\n",
    "1. When you don't want to modify `.grad` (e.g., to avoid accumulation).\n",
    "1. When computing gradients for multiple variables at once.\n",
    "1. When working with higher-order gradients (e.g., Hessians, Jacobians).\n",
    "1. When differentiating non-scalar outputs, using `grad_outputs`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "padl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
