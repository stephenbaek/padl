{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks - 1D Heat Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from utils import save_progress, make_gif, reset_parameters\n",
    "\n",
    "import IPython\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype=torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap on 1D Heat Diffusion\n",
    "\n",
    "In heat transfer, one-dimensional heat diffusion describes the distribution of temperature in a material over time due to thermal conduction. This phenomenon is governed by Fourier's law, which states that the heat flux $q$ is proportional to the negative temperature gradient, or:\n",
    "\n",
    "$q = -k\\frac{\\partial^2 u}{\\partial x^2}$\n",
    "\n",
    "where $k$ is the thermal conductivity of the material, $u$ is the temperature, and $x$ is the spatial coordinate.\n",
    "\n",
    "The conservation of energy leads to the heat diffusion equation. If we consider a material with density $\\rho$ and specific heat capacity $c_p$, the rate of change of temperature within the material is proportional to the divergence of the heat flux. This gives rise to the one-dimensional heat diffusion equation:\n",
    "\n",
    "$\\frac{\\partial u}{\\partial t} = \\kappa\\frac{\\partial^2 u}{\\partial x^2}$\n",
    "\n",
    "where $\\kappa = \\frac{k}{\\rho c_p}$ is the thermal diffusivity, a material property that quantifies the rate at which heat diffuses through the material.\n",
    "\n",
    "In an ideal case with constant thermal properties and no internal heat generation, the above equation describes how temperature evolves spatially and temporally. For example, if a rod is initially at a uniform temperature and then its ends are exposed to different fixed temperatures, the heat diffusion equation predicts how the temperature will vary along the rod and approach a steady-state distribution.\n",
    "\n",
    "Boundary and initial conditions are essential for solving the heat diffusion equation. Common boundary conditions include fixed temperatures (Dirichlet), fixed heat flux (Neumann), or convective heat transfer at the boundaries.\n",
    "\n",
    "Over time, the system approaches thermal equilibrium, where the temperature no longer changes with time ($\\frac{\\partial u}{\\partial t} = 0$). In this steady state, the heat diffusion equation reduces to:\n",
    "\n",
    "$\\frac{\\partial^2 u}{\\partial x^2} = 0$\n",
    "\n",
    "which indicates that the temperature varies linearly with position if the thermal conductivity is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Ground Truth (Analytic Solution)\n",
    "\n",
    "Here, we consider a 1D heat diffusion case with the following initial condition (initial heat distribution):\n",
    "\n",
    "$u(x,0) = \\sin (\\pi x)$\n",
    "\n",
    "Given the above initial condition, the analytic solution for the 1D heat equation is known to be:\n",
    "\n",
    "$u(x,t) = e^{-\\kappa \\pi^2 t}\\sin(\\pi x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "L = 1.0        # Length of the physical domain\n",
    "T = 2.0         # Total time\n",
    "Nx = 50        # Number of spatial points\n",
    "Nt = 100      # Number of time steps\n",
    "alpha = 0.05   # Thermal diffusivity\n",
    "dx = L / Nx    # Spatial step size\n",
    "dt = T / Nt    # Time step size\n",
    "\n",
    "# Discretize the spatial and time domain\n",
    "t = np.linspace(0, T, Nt+1)  # Time grid\n",
    "x = np.linspace(0, L, Nx+1)  # Spatial grid\n",
    "\n",
    "# Initialize temperature array\n",
    "u = np.zeros((Nt+1, Nx+1))   # u[n, i] corresponds to time step n and position i\n",
    "\n",
    "# Initial condition: u(0,x) = sin(pi * x)\n",
    "u[0, :] = np.sin(np.pi * x)\n",
    "\n",
    "for n in range(1, Nt+1):\n",
    "    # Analytical solution for the current time step\n",
    "    u[n, :] = np.exp(-alpha * (np.pi**2) * t[n]) * np.sin(np.pi * x)\n",
    "\n",
    "# Create a meshgrid for position and time\n",
    "xx, tt = np.meshgrid(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(\n",
    "        xx, tt, gt,\n",
    "        train_x=None, train_t=None, train_u=None,\n",
    "        colloc_x=None, colloc_t=None,\n",
    "        prediction=None,\n",
    "        iter=None\n",
    "    ):\n",
    "    # Create the surface plot\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d',computed_zorder=False)\n",
    "    ax.plot_surface(xx, tt, gt, cmap='viridis', alpha=0.8)\n",
    "    if not train_u is None:\n",
    "        ax.scatter(train_x, train_t, train_u, color='red')\n",
    "    if not colloc_x is None:\n",
    "        ax.scatter(colloc_x, colloc_t, np.zeros_like(colloc_t), color='green')\n",
    "    if not prediction is None:\n",
    "        wire = ax.plot_wireframe(xx, tt, prediction.reshape(xx.shape), color='deepskyblue', linewidth=1, zorder=2)\n",
    "\n",
    "    # Rotate the plot by setting elevation and azimuth\n",
    "    ax.view_init(elev=30, azim=60)  # Elevation and azimuth angles\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Position (x)')\n",
    "    ax.set_ylabel('Time (t)')\n",
    "    ax.set_zlabel('Temperature (u)')\n",
    "\n",
    "    iter_string = ''\n",
    "    if not iter is None:\n",
    "        iter_string = f' (Iteration={iter+1})'\n",
    "    ax.set_title('1D Heat Diffusion: Temperature Evolution' + iter_string)\n",
    "\n",
    "    # Adjust layout to avoid cutting off the z-axis label\n",
    "    plt.tight_layout()  # Automatically adjust the layout\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(xx, tt, u).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample training data\n",
    "\n",
    "Now to train a model, let's sample training data. Here, we emulate the scenario in which we only know the initial condition at $t=0$ and would like to solve the heat equation for unseen time $t>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data at t=0\n",
    "N_train = 15\n",
    "train_x = np.linspace(0, L, N_train)\n",
    "train_t = np.zeros_like(train_x)\n",
    "train_u = np.sin(np.pi * train_x)\n",
    "\n",
    "training_data = np.hstack(( np.expand_dims(train_t,-1), np.expand_dims(train_x,-1),np.expand_dims(train_u,-1)))\n",
    "\n",
    "# Sample collocation points\n",
    "N_COLLOCATION_POINTS = 12\n",
    "xx_colloc, tt_colloc = np.meshgrid(np.linspace(0,L,N_COLLOCATION_POINTS), np.linspace(0,T,N_COLLOCATION_POINTS))\n",
    "collocation_pts = torch.tensor(np.hstack((tt_colloc.reshape(-1,1), xx_colloc.reshape(-1,1))), dtype=dtype).requires_grad_(True)\n",
    "\n",
    "plot_result(xx, tt, u, train_x, train_t, train_u, xx_colloc, tt_colloc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Design\n",
    "\n",
    "Below is a simple MLP implementation as a starter. This will give you a pretty good solution to the diffusion problem, but you are strongly encouraged to play around with other more complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2, 32, dtype=dtype)  # input dim = 2 (t,x)\n",
    "        self.fc2 = nn.Linear(32, 32, dtype=dtype)  # hidden dims = 32, 32, 32\n",
    "        self.fc3 = nn.Linear(32, 32, dtype=dtype)  #\n",
    "        self.out = nn.Linear(32, 1, dtype=dtype)  # output dim = 1 (u)\n",
    "\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.SiLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.SiLU()(x)\n",
    "        x = self.fc3(x)\n",
    "        x = nn.SiLU()(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "model = Backbone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Finally, let's train the model. PINNs are difficult to converge, so you are going to need to tweak with things like loss weights, learning rates, etc. Also, make sure you run a sufficient number of iterations for the model the to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_parameters(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "files = []\n",
    "\n",
    "save_dir = 'results/heat1d'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "MAX_ITER = 30000\n",
    "for iter in range(MAX_ITER):\n",
    "    optimizer.zero_grad()\n",
    "    input = torch.tensor(training_data[:,:2], dtype=model.dtype)\n",
    "    output = torch.tensor(training_data[:,2:], dtype=model.dtype)\n",
    "    prediction = model(input)\n",
    "    data_loss = torch.mean((output-prediction)**2)\n",
    "    \n",
    "    prediction_colloc = model(collocation_pts)\n",
    "    # deriv = torch.autograd.grad(prediction_colloc, collocation_pts, torch.ones_like(prediction_colloc), retain_graph=True, create_graph=True)[0]\n",
    "    deriv = torch.autograd.grad(prediction_colloc, collocation_pts, torch.ones_like(prediction_colloc), retain_graph=True, create_graph=True)[0]\n",
    "    dt = deriv[:,0]\n",
    "    dx = deriv[:,1]\n",
    "    deriv2 = torch.autograd.grad(dx, collocation_pts, torch.ones_like(dx), create_graph=True)[0]\n",
    "    ddx = deriv2[:,1]\n",
    "    physics_loss = torch.mean((dt-alpha*ddx)**2)\n",
    "\n",
    "    loss = data_loss + physics_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"{iter+1}/{MAX_ITER} - loss: {loss.detach().numpy():.5e}, physics: {physics_loss.detach().numpy():.5e}\", end='\\r')\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if (iter+1) % 100 == 0: \n",
    "        \n",
    "        prediction = model(torch.tensor(np.hstack((tt.reshape(-1,1),xx.reshape(-1,1))), dtype=model.dtype)).detach()\n",
    "\n",
    "        plot_result(xx, tt, u, train_x, train_t, train_u, xx_colloc, tt_colloc, prediction, iter)\n",
    "        files.append(save_progress(save_dir, 'pinn', iter))\n",
    "        \n",
    "        if (iter+1) % 5000 == 0: plt.show()\n",
    "        else: plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate the training progress\n",
    "make_gif(files, save_dir + \"/pinn.gif\")\n",
    "IPython.display.Image(filename=save_dir + \"/pinn.gif\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "padl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
